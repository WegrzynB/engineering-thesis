{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym notatniku przejdę przez uczenie, walidację oraz predykcję modeli YOLOv8.\n",
    "\n",
    "Duża część kodu jest zapisana w Markdown'ie, ponieważ output generowany przez takie komórki jest ogromny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.benchmarks import benchmark\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "from IPython.display import Video, display, HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "    torch.cuda.set_device(0)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "except:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tworzenie plików konfiguracyjnych yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zacznijmy od utworzenia plików konfiguracyjnych (z rozszerzeniem .yaml).\n",
    "\n",
    "Znajdują się w nich informację o lokalizacji datasetów oraz nazwy klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config_yaml(file_name, path_dir, train_dir, val_dir, test_dir = None, classes_fn = None):\n",
    "    if not os.path.isdir(path_dir):\n",
    "        print(f\"{path_dir} is not a directory\")\n",
    "        return None\n",
    "    if not os.path.isdir(os.path.join(path_dir, train_dir)):\n",
    "        print(f\"{os.path.join(path_dir, train_dir)} is not a directory\")\n",
    "        return None\n",
    "    if not os.path.isdir(os.path.join(path_dir, val_dir)):\n",
    "        print(f\"{os.path.join(path_dir, val_dir)} is not a directory\")\n",
    "        return None\n",
    "    if test_dir is not None:\n",
    "        if not os.path.isdir(os.path.join(path_dir, test_dir)):\n",
    "            print(f\"{os.path.join(path_dir, test_dir)} is not a directory\")\n",
    "            return None\n",
    "    else:\n",
    "        test_dir = ''\n",
    "    if classes_fn is not None:\n",
    "        if not os.path.isfile(classes_fn):\n",
    "            print(f\"Classes file {classes_fn} is not a file\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Classes file can't be 'None'\")\n",
    "        return None\n",
    "        \n",
    "\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.write('path: ')\n",
    "        f.write(str(path_dir))\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.write('train: ')\n",
    "        f.write(str(train_dir))\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.write('val: ')\n",
    "        f.write(str(val_dir))\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.write('test: ')\n",
    "        f.write(str(test_dir))\n",
    "        f.write(' # Optional\\n\\n')\n",
    "\n",
    "        f.write('# Classes\\n')\n",
    "        f.write('names: ' + '\\n')\n",
    "        with open(classes_fn, 'r') as f_classes:\n",
    "            for i, line in enumerate(f_classes):\n",
    "                f.write(f'  {i}: {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/config_photos.yaml'\n",
    "path_dir = 'C:/Users/Bartek/Documents/Studia/INZ/Program/yolov8_models/data'\n",
    "train_dir = 'photos/train'\n",
    "val_dir = 'photos/val'\n",
    "test_dir = 'photos/test'\n",
    "classes_fn = 'data/cards.names'\n",
    "\n",
    "create_config_yaml(file_name, path_dir, train_dir, val_dir, test_dir, classes_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/config_videos.yaml'\n",
    "path_dir = 'C:/Users/Bartek/Documents/Studia/INZ/Program/yolov8_models/data'\n",
    "train_dir = 'videos/train'\n",
    "val_dir = 'videos/val'\n",
    "test_dir = 'videos/test'\n",
    "classes_fn = 'data/cards.names'\n",
    "\n",
    "create_config_yaml(file_name, path_dir, train_dir, val_dir, test_dir, classes_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uczenie modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając pliki konfiguracyjne oraz wcześniej wygenerowane datasety, możemy przejść do utworzenia modeli, omówienia parametrów oraz treningu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie modeli\n",
    "\n",
    "Utwórzmy nowe modele od zera (bazują one na architekturach już gotowych modeli, ale nie są pretrenowane). Modele te to *yolov8m*, czyli takie średniej wielkości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#####\n",
    "model_photos = YOLO(\"yolov8m.yaml\")\n",
    "\n",
    "model_videos = YOLO(\"yolov8m.yaml\")\n",
    "#####\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametry i metryki trenowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zanim przejdziemy do trenowania, omówmy najważniejsze parametry występujące podczas treningu:\n",
    "\n",
    "- **data** - Ścieżka do pliku konfiguracyjnego zestawu danych (np. coco8.yaml). Plik ten zawiera parametry specyficzne dla zbioru danych, w tym ścieżki do danych szkoleniowych i walidacyjnych, nazwy klas i liczbę klas.\n",
    "\n",
    "- **epochs** - Całkowita liczba epok szkoleniowych. Każda epoka reprezentuje pełne przejście przez cały zestaw danych. Dostosowanie tej wartości może wpłynąć na czas trwania treningu i wydajność modelu.\n",
    "\n",
    "- **project** - Nazwa katalogu projektu, w którym zapisywane są wyniki treningu. Umożliwia zorganizowane przechowywanie różnych eksperymentów.\n",
    "\n",
    "- **name** - Nazwa treningu. Służy do tworzenia podkatalogu w folderze projektu, w którym przechowywane są dzienniki treningowe i wyniki.\n",
    "\n",
    "- **seed** - Ustawia losowe ziarno dla treningu, zapewniając powtarzalność wyników między przebiegami z tymi samymi konfiguracjami.\n",
    "\n",
    "- **batch** - Ilość obrazów przetwarzanych jednocześnie podczas treningu.\n",
    "\n",
    "- **imgsz** - Docelowy rozmiar obrazu do treningu. Wszystkie obrazy są skalowane do tego wymiaru przed wprowadzeniem ich do modelu. Wpływa na dokładność modelu i złożoność obliczeniową.\n",
    "\n",
    "- **workers** - Liczba wątków roboczych do ładowania danych. Wpływa na szybkość wstępnego przetwarzania danych i wprowadzania ich do modelu, co jest szczególnie przydatne w konfiguracjach z wieloma procesorami graficznymi.\n",
    "\n",
    "- **lr0** - Początkowa szybkość uczenia się (dla optymizer: SGD=1E-2, Adam=1E-3). Dostosowanie tej wartości ma kluczowe znaczenie dla procesu optymalizacji, wpływając na szybkość aktualizacji wag modelu.\n",
    "\n",
    "- **lrf** - Końcowy wskaźnik uczenia się jako ułamek wskaźnika początkowego = (lr0 * lrf), używany w połączeniu z harmonogramami w celu dostosowania wskaźnika uczenia się w czasie.\n",
    "\n",
    "- **optimizer** - Wybór optymalizatora do szkolenia. Opcje obejmują SGD, Adam, AdamW, NAdam, RAdam, RMSProp itp. lub auto do automatycznego wyboru na podstawie konfiguracji modelu. Wpływa na szybkość i stabilność zbieżności.\n",
    "\n",
    "- **momentum** - Współczynnik pędu dla SGD lub beta1 dla optymalizatorów Adam, wpływający na włączenie przeszłych gradientów do bieżącej aktualizacji.\n",
    "\n",
    "- **amp** - Umożliwia automatyczne szkolenie z mieszaną precyzją (AMP), zmniejszając zużycie pamięci i prawdopodobnie przyspieszając szkolenie przy minimalnym wpływie na dokładność.\n",
    "\n",
    "- **val** - Umożliwia walidację podczas treningu, pozwalając na okresową ocenę wydajności modelu na oddzielnym zbiorze danych.\n",
    "\n",
    "- **patience** - Liczba epok, w których należy odczekać bez poprawy wskaźników walidacji przed wcześniejszym zatrzymaniem treningu. Pomaga zapobiegać nadmiernemu dopasowaniu, zatrzymując szkolenie, gdy wydajność osiąga plateau.\n",
    "\n",
    "- **warmup_epochs** - Liczba epok dla rozgrzewki tempa uczenia, stopniowe zwiększanie tempa uczenia od niskiej wartości do początkowego tempa uczenia w celu wczesnego ustabilizowania treningu.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "Dodatkowo przejdźmy przez metryki wykorzystane w uczeniu:\n",
    "- **P (Precision)**: Dokładność wykrytych obiektów, wskazująca, ile wykryć było prawidłowych.\n",
    "\n",
    "- **R (Recall)**: Zdolność modelu do identyfikacji wszystkich instancji obiektów na obrazach.\n",
    "\n",
    "- **mAP50**: Średnia precyzja obliczona przy progu intersection over union (IoU) wynoszącym 0,50. Jest to miara dokładności modelu uwzględniająca tylko \"łatwe\" wykrycia.\n",
    "\n",
    "- **mAP75**: Średnia precyzja obliczona przy progu intersection over union (IoU) wynoszącym 0,75.\n",
    "\n",
    "- **mAP50-95**: Średnia średnia precyzja obliczona przy różnych progach IoU, w zakresie od 0,50 do 0,95. Daje to kompleksowy obraz wydajności modelu na różnych poziomach trudności wykrywania. Jest to \"najważniejsza\" metryka, ponieważ w oparciu o nią będziemy dokonywać oceny modelu oraz ona decyduje o zatrzymaniu uczenia modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trenowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zacznijmy trenowanie na dwóch wcześniej utworzonych datasetach (*photos* i *videos*). Trening odbędzie się w 2 sesjach z lekko zmenionymi parametrami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trening odbywa się na tzw. batch'ach, czyli połączonych obrazach (w ilości równej *batch_size*), na których zaznaczone są współrzędne informacji o karcie, wraz z indeksem klasy, z której jest dana karta. Poniżej przykład części batcha w dwóch obrazach:\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 45%;\">\n",
    "    <img src='models/photos/train_/train_batch0.jpg' width='700' alt=\"Train Batch 1\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 45%;\">\n",
    "    <img src='models/photos/train_/train_batch1.jpg' width='700' alt=\"Train Batch 2\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natomiast po zakończeniu treningu przeprowadzamy walidację na zbiorze walidacyjnym. Obrazy z tamtąd też łączymy w batch'e i na nich określamy prawdopodobieństwo (tutaj zaokrąglane do jednego miejsca po przecinku), że to jest dana klasa:\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 45%;\">\n",
    "    <img src='models/photos/train_/val_batch0_pred.jpg' width='700' alt=\"Val Batch 1\"/>\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 45%;\">\n",
    "    <img src='models/photos/train_/val_batch1_pred.jpg' width='700' alt=\"Val Batch 2\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sesja 1\n",
    "\n",
    "```python\n",
    "model_photos.train(\n",
    "    data = 'data/config_photos.yaml',\n",
    "    epochs = 100,\n",
    "    project = 'models/photos',\n",
    "    name = 'train_',\n",
    "    seed = 2024,\n",
    "    batch = 0.8,    # 80% of GPU VRAM = 84\n",
    "    imgsz = 640,\n",
    "    save = True,\n",
    "    workers = 8,\n",
    "    plots = True,\n",
    "    lr0 = 0.001,\n",
    "    lrf = 0.01,\n",
    "    momentum = 0.9,\n",
    "    amp = True,\n",
    "    val = True,\n",
    "    optimizer = 'AdamW',\n",
    "    patience = 10,\n",
    "    warmup_epochs = 0,\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "Zatrzymano po 69 epokach z powodu braku poprawy. Jako że **patience = 10**, to najlepsze wagi zaobserwowano w 59 epoce i to właśnie od niej rozpoczniemy sesję drugą uczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sesja 2\n",
    "\n",
    "Podczas sesji drugiej bierzemy niższy **lr0 = 0.0001** oraz **imgsz = 1024**, czyli uczymy wolniej na obrazach wyższej rozdzielczości.\n",
    "\n",
    "```python\n",
    "model_photos.train(\n",
    "    data = 'data/config_photos.yaml',\n",
    "    epochs = 100,\n",
    "    project = 'models/photos',\n",
    "    name = 'train_2',\n",
    "    seed = 2024,\n",
    "    batch = 0.8,    # 80% of GPU VRAM = 33\n",
    "    imgsz = 1024,\n",
    "    save = True,\n",
    "    workers = 8,\n",
    "    plots = True,\n",
    "    lr0 = 0.0001,\n",
    "    lrf = 0.01,\n",
    "    momentum = 0.9,\n",
    "    amp = True,\n",
    "    val = True,\n",
    "    optimizer = 'AdamW',\n",
    "    patience = 10,\n",
    "    warmup_epochs = 0,\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "Ta sesja została zatrzymana po 18 epokach, więc nasz finalny model pochodzi z 8 epoki tego treningu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sesja 1\n",
    "\n",
    "```python\n",
    "model_videos.train(\n",
    "    data = 'data/config_videos.yaml',\n",
    "    epochs = 100,\n",
    "    project = 'models/videos',\n",
    "    name = 'train_',\n",
    "    seed = 2024,\n",
    "    batch = 0.8,    # 80% of GPU VRAM = 84\n",
    "    imgsz = 640,\n",
    "    save = True,\n",
    "    workers = 8,\n",
    "    plots = True,\n",
    "    lr0 = 0.001,\n",
    "    lrf = 0.01,\n",
    "    momentum = 0.9,\n",
    "    amp = True,\n",
    "    val = True,\n",
    "    optimizer = 'AdamW',\n",
    "    patience = 10,\n",
    "    warmup_epochs = 0,\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "Zatrzymano po 68 epokach, co oznacza, że najlepsze wagi otrzymaliśmy po 58 epoce. Tak samo jak w przypadku *photos* od niej wznowimy trening z nowymi parametrami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sesja 2\n",
    "\n",
    "Podczas sesji drugiej bierzemy niższy **lr0 = 0.0001** oraz **imgsz = 1024**, czyli uczymy wolniej na obrazach wyższej rozdzielczości.\n",
    "\n",
    "```python\n",
    "model_videos.train(\n",
    "    data = 'data/config_videos.yaml',\n",
    "    epochs = 100,\n",
    "    project = 'models/videos',\n",
    "    name = 'train_2',\n",
    "    seed = 2024,\n",
    "    batch = 0.8,    # 80% of GPU VRAM = 33\n",
    "    imgsz = 1024,\n",
    "    save = True,\n",
    "    workers = 8,\n",
    "    plots = True,\n",
    "    lr0 = 0.0001,\n",
    "    lrf = 0.01,\n",
    "    momentum = 0.9,\n",
    "    amp = True,\n",
    "    val = True,\n",
    "    optimizer = 'AdamW',\n",
    "    patience = 10,\n",
    "    warmup_epochs = 0,\n",
    "    )\n",
    "```\n",
    "\n",
    "\n",
    "Zatrzymanie drugiej sesji nastąpiło po 70 epokach, więc finalny model to ten po 60 epokach treningu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie treningu (wykresy i opis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przejdźmy teraz do omówienia wyników treningu i ich interpretacji.\n",
    "\n",
    "Najpierw jednak omówmy co oznaczają składniki ogólnej wartości strat:\n",
    "- box_loss - mierzy błąd w przewidywaniu współrzędnych obwiedni. Zachęca model do dostosowania przewidywanych obwiedni do obwiedni rzeczywistych.\n",
    "\n",
    "- cls_loss - określa ilościowo błąd w przewidywaniu klasy obiektu dla każdego obwiedni. Zapewnia to, że model dokładnie identyfikuje kategorię obiektu.\n",
    "\n",
    "- dfl_loss - to wyspecjalizowany składnik strat, który pomaga poprawić wykrywanie obiektów w scenariuszach z nieostrymi lub rozmytymi obrazami. Zachęca model do skupienia się na poprawie wykrywania w takich trudnych warunkach.\n",
    "\n",
    "\n",
    "W uczeniu wykorzystujemy 2 zbiory:\n",
    "- train - zbiór treningowy, na którym nasz zbiór się uczy\n",
    "\n",
    "- val - zbiór walidacyjny, dzięki któremu możemy uniknąć zjawiska overfitting'u oraz pomaga on nam we wcześniejszym przerwaniu treningu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy najpierw wykresy rezultatu treningów:\n",
    "\n",
    "<img src='notebooks/plots/photos/all_plots.png' width='1200' alt=\"Train Results Photos\"/>\n",
    "\n",
    "Widzimy, że trening przebiega dość sprawnie. Po przejściu do sesji 2 znacznie poprawiają się wartości opierające się na zbiorze validacyjnym co jest bardzo pożądane.\n",
    "\n",
    "Teraz jeszcze rzućmy okiem na wykres 'metrics_mAP50-95':\n",
    "\n",
    "<img src='notebooks/plots/photos/separated/metrics_mAP50-95(B).png' width='1000' alt=\"Train Precision Photos\"/>\n",
    "\n",
    "Maksymalna wartość to ponad **97%** na zbiorze walidacyjnym, co jest świetnym wynikiem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Znomu zerknijmy na wszystkie wykresy:\n",
    "\n",
    "<img src='notebooks/plots/videos/all_plots.png' width='1200' alt=\"Train Results Videos\"/>\n",
    "\n",
    "Uczenie na zbiorze treningowym pzebiega bardzo sprawnie, jednak przy przejściu do sesji 2 możemy zaobserwować znaczny wzrost wartości strat na zbiorze walidacyjnym\n",
    "\n",
    "Oraz na wykres 'metrics_mAP50-95':\n",
    "\n",
    "<img src='notebooks/plots/videos/separated/metrics_mAP50-95(B).png' width='1000' alt=\"Train Precision Videos\"/>\n",
    "\n",
    "Tutaj maksimum wynosi niecałe **91%**, będące nadal dobrym wynikiem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Omówienie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że **photos** mają delikatnie lepszy model niż **videos**. Przejawia się to mniejszymi wartościami straty oraz lepszą średnią precyzją.\n",
    "\n",
    "Pomimo, że parametry modeli były bardzo zbliżone, tak rezultaty są dość różne. Jest to spowodowane oczywiście różnymi zbiorami treningowymi i walidacyjnymi. \n",
    "\n",
    "Możemy wysnuć wniosek, że *model_photos* miał dane \"lepszej\" jakości i model jest bardziej stabilny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 'photos_large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystając z tego, że **photos** mają \"lepsze\" dane, utwórzmy model *large* opierający się na danych z **photos**.\n",
    "\n",
    "Modele typu *large* charakteryzują się lepszą dokładnością i stabilnością, kosztem delikatnego opóźnienia oraz dłuższego czesu uczenia.\n",
    "\n",
    "```python\n",
    "#####\n",
    "model_photos_large = YOLO(\"yolov8l.yaml\")\n",
    "#####\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sesja 1\n",
    "```python\n",
    "model_photos_large.train(\n",
    "    data = 'data/config_photos.yaml',\n",
    "    epochs = 100,\n",
    "    project = 'models/photos_large',\n",
    "    name = 'train_',\n",
    "    seed = 2024,\n",
    "    batch = 0.9,    # 90% of GPU VRAM = 60\n",
    "    imgsz = 640,\n",
    "    save = True,\n",
    "    workers = 8,\n",
    "    plots = True,\n",
    "    lr0 = 0.001,\n",
    "    lrf = 0.01,\n",
    "    momentum = 0.9,\n",
    "    amp = True,\n",
    "    val = True,\n",
    "    optimizer = 'AdamW',\n",
    "    patience = 20,\n",
    "    )\n",
    "```\n",
    "\n",
    "Zatrzymano po 55 epokach z powodu braku poprawy. **patience = 20**, więc najlepsze wagi zaobserwowano w 35 epoce i to właśnie od niej rozpoczniemy sesję drugą uczenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sesja 2\n",
    "\n",
    "Podczas sesji drugiej bierzemy niższy **lr0 = 0.0001** oraz **imgsz = 1024**, czyli uczymy wolniej na obrazach wyższej rozdzielczości. Dodatkowo zmniejszamy **patience = 10** oraz **epochs = 30**.\n",
    "\n",
    "```python\n",
    "model_photos_large.train(\n",
    "    data = 'data/config_photos.yaml',\n",
    "    epochs = 30,\n",
    "    project = 'models/photos_large',\n",
    "    name = 'train_2',\n",
    "    seed = 2024,\n",
    "    batch = 0.9,    # 90% of GPU VRAM = 23\n",
    "    imgsz = 1024,\n",
    "    save = True,\n",
    "    workers = 8,\n",
    "    plots = True,\n",
    "    lr0 = 0.0001,\n",
    "    lrf = 0.01,\n",
    "    momentum = 0.9,\n",
    "    amp = True,\n",
    "    val = True,\n",
    "    optimizer = 'AdamW',\n",
    "    patience = 10,\n",
    "    )\n",
    "```\n",
    "\n",
    "Zatrzymanie drugiej sesji nastąpiło po 15 epokach, więc finalny model to ten po 5 epokach treningu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spójrzmy na wykresy rezultatu treningów:\n",
    "\n",
    "<img src='notebooks/plots/photos_large/all_plots.png' width='1200' alt=\"Train Results Photos Large\"/>\n",
    "\n",
    "Trening przebiega bardzo dobrze. Po rozpoczęciu na sesję 2 widzimy wzrost wartości strat (co jest spowodowane domyślną wartością *warmup_epochs = 3*), które szybko zaczynają opadać, a precyzja rośnie.\n",
    "\n",
    "Teraz jeszcze rzućmy okiem na wykres 'metrics_mAP50-95':\n",
    "\n",
    "<img src='notebooks/plots/photos_large/separated/metrics_mAP50-95(B).png' width='1000' alt=\"Train Precision Photos Large\"/>\n",
    "\n",
    "Maksymalna wartość to prawie **96%** na zbiorze walidacyjnym, co stanowi bardzo dobry wynik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walidacja oraz testy modeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gdy tworzyliśmy nasze datasety to stanowiły one zbiory: treningowe, walidacyjne i testowe. Jak dotąd wykorzystywaliśmy tylko zbiory treningowe i walidacyjne. Do finalnej walidacji modeli oraz określenia, który z nich jest najlepszy wykorzystamy zbiory testowe. \n",
    "\n",
    "W celu uczciwego porównania modeli utworzymy jeden zbiór testowy łączący zbiory testowe **photos** i **videos**, czyli 30000 obrazów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "val_photos = model_photos.val(split='test', plots=True, project='models/photos', name='val', half=False)\n",
    "val_videos = model_videos.val(split='test', plots=True, project='models/videos', name='val', half=False)\n",
    "val_photos_large = model_photos_large.val(split='test', plots=True, project='models/photos_large', name='val', half=False)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po wykonaniu walidacji modeli na połączonym zbiorze testowym zapiszemy ich wyniki za pomocą funkcji:\n",
    "\n",
    "```python\n",
    "def return_metrics(results, save=False, save_dir='runs'):\n",
    "    metrics = {}\n",
    "    metrics['map'] = results.box.map             # map50-95\n",
    "    metrics['map50'] = results.box.map50         # map50\n",
    "    metrics['map75'] = results.box.map75         # map75\n",
    "    metrics['maps'] = list(results.box.maps)     # a list contains map50-95 of each category\n",
    "    metrics['mp'] = results.box.mp               # P\n",
    "    metrics['mr'] = results.box.mr               # R\n",
    "\n",
    "     if save:\n",
    "     with open(os.path.join(save_dir, 'results.txt'), 'w') as file:\n",
    "        file.write(str(metrics))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "metrics_photos = return_metrics(val_photos, True, 'models/photos/val')\n",
    "metrics_videos = return_metrics(val_videos, True, 'models/videos/val')\n",
    "metrics_photos_large = return_metrics(val_photos_large, True, 'models/photos_large/val')\n",
    "```\n",
    "\n",
    "Po wykonaniu powyższego kodu możemy zobaczyć finalne wyniki walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_val_results(val_txt):\n",
    "    with open(val_txt, 'r') as file:\n",
    "        results = ast.literal_eval(file.read())\n",
    "\n",
    "    print('mAP 50-95:', round(results['map'], 6))\n",
    "    print('mAP 50:', round(results['map50'], 6))\n",
    "    print('mAP 75:', round(results['map75'], 6))\n",
    "    print('mP:', round(results['mp'], 6))\n",
    "    print('mR:', round(results['mr'], 6))\n",
    "\n",
    "    with open('data/cards.names', 'r') as f:\n",
    "        cards = f.read().split(\"\\n\")\n",
    "    cards = [c for c in cards if c!='']\n",
    "\n",
    "    df_map = pd.DataFrame({'Karta': cards, 'mAP 50-95': results['maps']})\n",
    "\n",
    "    return df_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dzięki tej funkcji zobaczymy średnie wartości metryk oraz najmniejsze i największe precyzje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_photos = print_val_results('models/photos/val/results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_photos.sort_values('mAP 50-95').head().style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_photos.sort_values('mAP 50-95', ascending=False).head().style.hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='models/photos/val/confusion_matrix.png' width='800' alt=\"Confusion Matrix Photos\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po przedstawionych parametrach widzimy, że model bardzo dobrze poradził sobie z walidacją na danych testowych.\n",
    "\n",
    "mAP 50-95 na poziomie 95% jest świetnym wynikiem. Jedyną rzeczą mogącą niepokoić jest niższa precyzja na kartach specjalnych (np. Król Czerwo).\n",
    "\n",
    "Macierz pomyłem obrazuje nam drobne pomyłki w rozpoznawaniu klas lub to czy katę w ogólę wykryto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_videos = print_val_results('models/videos/val/results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_videos.sort_values('mAP 50-95').head().style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_videos.sort_values('mAP 50-95', ascending=False).head().style.hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='models/videos/val/confusion_matrix.png' width='800' alt=\"Confusion Matrix Videos\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W przypadku modelu 'videos' sprawa ma się już trochę gorzej. Dalej jest to niezły model, ale nasza precyzja mocno spada (średnio o ok. 5%), co jest zauważalne w maksymalnych wartościach predykcji. Macierz pomyłem pokazuje również, że model trochę się myli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Photos_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_photos_large = print_val_results('models/photos_large/val/results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_photos_large.sort_values('mAP 50-95').head().style.hide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map_photos_large.sort_values('mAP 50-95', ascending=False).head().style.hide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='models/photos_large/val/confusion_matrix.png' width='800' alt=\"Confusion Matrix Photos Large\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatni model 'photos_large' prezentuje się bardzo dobrze. Osiąga największą średnią precyzję oraz tylko dwie klasy są rozpoznawane w mniej niż 90% przypadków (tak samo jak model 'photos').\n",
    "\n",
    "Macierz pomyłem pokazuję nam niską ilość pomyłek w rozpoznawaniu kart.\n",
    "\n",
    "Także model ten wydaję się ulepszeniem modelu 'photos'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wybór najlepszego modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zapoznaniu się z rezultatami wykonanych walidacji przechodzimy do następnego kroku. Musimy zdecydować, którego modelu użyjemy podczas tworzenia aplikacji, czyli w prostych słowach: musimy zdecydować, który model jest najlepszy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zacznijmy od spójrzenia na średnie precyzję modeli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_side_by_side(dfs:list, captions:list):\n",
    "    \"\"\"Display tables side by side to save vertical space\n",
    "    Input:\n",
    "        dfs: list of pandas.DataFrame\n",
    "        captions: list of table captions\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    combined = dict(zip(captions, dfs))\n",
    "    for caption, df in combined.items():\n",
    "        output += df.style.set_table_attributes(\"style='display:inline'\").set_caption(caption)._repr_html_()\n",
    "        output += \"&emsp;&emsp;\"\n",
    "    display(HTML(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_side_by_side([df_map_photos, df_map_photos_large, df_map_videos], [\"'Photos Medium'\", \"'Photos Large'\", \"'Videos'\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutimedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zobaczeniu precyzji danych modeli na poszczególnych kartach, możemy przejść do wykonania predykcji na realnych zdjęciach oraz filmach wideo. W tym celu utworzymy dwie funkcję:\n",
    "- **predict_test_data** - odpowiada za wykonanie predykcji na wszystkich plikach w danym folderze przez zadany model\n",
    "- **convert_avi_mp4** - konwertuję pliki wideo z rozszerzeniem *.avi* (domyśle rozszerzenie predykcji na wideo) do *.mp4*. Jest to przydatne do wyświetlania wideo w notatniku\n",
    "\n",
    "```python\n",
    "def predict_test_data(model, test_data='../data/test_data', project='predict_test_data', name='predict', img_size = 640):\n",
    "    if os.path.exists(project):\n",
    "        shutil.rmtree(project)\n",
    "        \n",
    "    results = model(test_data, stream=True, save=True, project=project, name=name, verbose=False, imgsz=img_size)\n",
    "    for r in results:\n",
    "        boxes = r.boxes  # Boxes object for bbox outputs\n",
    "        masks = r.masks  # Masks object for segment masks outputs\n",
    "        probs = r.probs  # Class probabilities for classification outputs\n",
    "        \n",
    "\n",
    "def convert_avi_mp4(folders):\n",
    "    for fol in folders:\n",
    "        for video_file in glob(os.path.join(fol)+\"/*.avi\"):\n",
    "            moviepy.VideoFileClip(video_file).write_videofile(video_file.replace(\".avi\", \".mp4\"), verbose=False, logger=None)\n",
    "            os.remove(video_file)\n",
    "\n",
    "    print('Zakończono konwersję')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz tylko pozostało nam wykonać poniższy kod, aby otrzymać nasze predykcje:\n",
    "\n",
    "```python\n",
    "model_photos = YOLO('../models/photos/train_2/weights/best.pt')\n",
    "test_data = '../data/test_data'\n",
    "project = 'predict_test_data/photos'\n",
    "name = 'predict'\n",
    "predict_test_data(model_photos, test_data, project, name)\n",
    "\n",
    "\n",
    "model_videos = YOLO('../models/videos/train_2/weights/best.pt')\n",
    "test_data = '../data/test_data'\n",
    "project = 'predict_test_data/videos'\n",
    "name = 'predict'\n",
    "predict_test_data(model_videos, test_data, project, name)\n",
    "\n",
    "\n",
    "model_photos_large = YOLO('../models/photos_large/train_2/weights/best.pt')\n",
    "test_data = '../data/test_data'\n",
    "project = 'predict_test_data/photos_large'\n",
    "name = 'predict'\n",
    "predict_test_data(model_photos_large, test_data, project, name)\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "Oraz ten, żeby przekonwertować pliki wideo:\n",
    "\n",
    "```python\n",
    "folders = [\n",
    "    'final_predict/pt/predict',\n",
    "    'final_predict/onnx/predict',\n",
    "    'final_predict/tflite/predict',\n",
    "]\n",
    "\n",
    "convert_avi_mp4(folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz zerknijmy jak te predykcje wyglądają."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zdjęcia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">Photos Medium</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_1.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">Photos Large</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_1.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">Videos</p>\n",
    "    <img src=\"notebooks/predict_test_data/videos/predict/img_1.jpg\" alt=\"Image 3\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_3.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_3.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/videos/predict/img_3.jpg\" alt=\"Image 3\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_5.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_5.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/videos/predict/img_5.jpg\" alt=\"Image 3\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "&emsp;&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_6.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_6.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 30%;\">\n",
    "    <img src=\"notebooks/predict_test_data/videos/predict/img_6.jpg\" alt=\"Image 3\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Na powyższych obrazach widzimy, że predykcje wykonywane przez model 'videos' odstają dokładnością. Dobitnie widać to na ostatnim zdjęciu, gdzie 'videos' nawet nie rozpoznaje kart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zapoznaniu się z walidacją, tabelą przedstawiającą średnią precyzję danej karty oraz powyższymi predykcjami możemy stwierdzić, że najgorsze wyniki miał model 'videos', więc nie będzie on brał udziału w walce o bycie najlepszym modelem.\n",
    "\n",
    "Zostają nam modele oparcie na datasecie 'photos':\n",
    "- *yolov8m* - model 'medium' (średniej wielkości)\n",
    "- *yolov8l* - model 'large' (duży)\n",
    "\n",
    "Na pierwszy rzut oka model 'large' wydaję się naturalnym rozwinięciem modelu 'medium', co zdają się potwierdzać wartości metryk. Faktycznie lekko lepiej radzi sobie większy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jednak, żeby mieć pewność zobaczmy jak modele prezentują się na kolejnych przygotowanych zdjęciach oraz filmach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">Photos Medium</p>\n",
    "    <p style=\"text-align: center;\">#1</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_2.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">Photos Large</p>\n",
    "    <p style=\"text-align: center;\">#1</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_2.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Widzimy, że 'Medium' ma problemy z oświetloną kartą oraz prawdopodobieństwa są niższe\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#2</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_4.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#2</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_4.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "'Medium' radzi sobie troszkę gorzej, ale widzi jedą predykcję tam, gdzie 'Large' popełnia błąd\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#3</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_7.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#3</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_7.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Tutaj widzimy, że jest dość równo, z lekką przewagą 'Large'\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#4</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_8.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#4</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_8.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Tutaj możemy zobaczyć, że 'Large' nie jest idealny i myli się w detekcji mocno pochylonej karty ('Medium' też popełnia ten błąd)\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#5</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_9.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#5</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_9.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Kolejna zła detekcja od 'Medium'. Tym razem obie nie wykrywają mocno pochylonej karty\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#6</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_10.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#6</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_10.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Dobre predykcje, miejscami 'Medium' wykonuję detekcję na niskim prawdopodobieństwie\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#7</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_11.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#7</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_11.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Solidne predykcje, nawet jedna karta jest lepiej wykryta na 'Medium'\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#8</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_12.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#8</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_12.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Przez padanie światła obydwa modele gubią detekcję, jednak znowu 'Large' wygląda lepiej\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#9</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_13.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#9</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_13.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "W obydwu przypadkach bardzo dobre detekcje\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#10</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_14.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#10</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_14.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Zdjęcie w ciemniejszym pomieszczeniu pokazuję, że 'Large' w większości przypadków radzi sobie lepiej\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#11</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_15.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#11</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_15.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Oba modele dokonują dobrych detekcji, dodatkowo widzą niewidoczne dla sobie nawzajem karty\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#12</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_16.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#12</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_16.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Obydwa modele nie widzą jednej karty\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#13</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_17.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#13</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_17.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Dobre predykcję, 'Large' widzi mocno pochyloną kartę\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#14</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_18.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#14</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_18.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Solidne detekcje\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#15</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_19.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#15</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_19.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "W jednym przypadku 'Large' się myli, a 'Medium' nawet nie wykrywa rogu karty\n",
    "\n",
    "&emsp;\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#16</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos/predict/img_20.jpg\" alt=\"Image 1\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 40%;\">\n",
    "    <p style=\"text-align: center;\">#16</p>\n",
    "    <img src=\"notebooks/predict_test_data/photos_large/predict/img_20.jpg\" alt=\"Image 2\" style=\"height: 350px;\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Prawdiłowe detekcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po przeanalizowaniu zdjęć możemy stwierdzić lekką przewagę po stronie modelu 'Photos Large'.\n",
    "\n",
    "Teraz możemy przejść do analizy nagranych filmów wideo z predykcjami, aby finalnie wybrać najlepszy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filmy wideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "def show_videos_html(labels, numbers_in_line, files, description):\n",
    "  lines = len(files)//numbers_in_line\n",
    "  html_str = \"\"\"\"\"\"\n",
    "\n",
    "  if numbers_in_line == 2:\n",
    "     height = '400px'\n",
    "     width = '45%'\n",
    "  elif numbers_in_line == 3:\n",
    "     height = '300px'\n",
    "     width = '32%'\n",
    "\n",
    "  for k in range(lines):\n",
    "    filepaths = files[k*numbers_in_line : (k+1)*numbers_in_line]\n",
    "    \n",
    "    html_str += \"\"\"<div style=\"display: flex; font-size: 17px\">\"\"\"\n",
    "    for i, file in enumerate(filepaths):\n",
    "        mp4 = open(file,'rb').read()\n",
    "        data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "        html_str += \"\"\"<div style=\"text-align: center; width: %s;\">\"\"\" % str(width)\n",
    "        if k == 0:\n",
    "          html_str += \"\"\"<p style=\"font-weight: bold; text-align: center;\">%s</p>\"\"\" % labels[i]\n",
    "        html_str += \"\"\"<p style=\"text-align: center;\">#%s</p>\"\"\" % str(k+1)\n",
    "        html_str += \"\"\"\n",
    "        <video controls style=\"height: %s;\" src=\"%s\"></video>\n",
    "        \"\"\" % (str(height), data_url)\n",
    "        html_str += \"\"\"</div>\"\"\"\n",
    "        \n",
    "    html_str += \"\"\"</div>\"\"\"\n",
    "    html_str += \"\"\"<p style=\"font-size: 17px\">%s</p>\"\"\" % description[k]\n",
    "    html_str += \"\"\"&emsp;\"\"\"\n",
    "\n",
    "  return HTML(html_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "labels = ['Photos Medium', 'Photos Large']\n",
    "\n",
    "numbers_in_line = 2\n",
    "\n",
    "files = [\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_1.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_1.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_2.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_2.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_3.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_3.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_4.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_4.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_5.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_5.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_6.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_6.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_7.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_7.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_8.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_8.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_9.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_9.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_10.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_10.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_11.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_11.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_12.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_12.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_13.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_13.mp4\",\n",
    "    \"notebooks/predict_test_data/photos/predict/vid_14.mp4\", \"notebooks/predict_test_data/photos_large/predict/vid_14.mp4\",\n",
    "]\n",
    "\n",
    "description = [\n",
    "    \"Oba modele wydają się być bardzo szybkie, choć momentami gubią karty\",\n",
    "    \"Tutaj pojawiają się błędy z 'Medium': złe predykcję i brak stabilności\",\n",
    "    \"Znowu 'Medium' ma problemy, teraz duża liczba detekcji sprawia problem modelowi\",\n",
    "    \"Na tym filmie sprawa ma się już lepiej, zarówno stabilność jak i dokładność są lepsze\",\n",
    "    \"Na białym tle oba modele mają problemy, lecz to 'Large' po raz kolejny radzi sobie lepiej\",\n",
    "    \"Tutaj oba modele spisują się poniżej oczekiwań\",\n",
    "    \"Przy wyższej szybkości przesuwania kamery obydwa modele mają problemy\",\n",
    "    \"Szybkość i dokładność predykcji jest tutaj bardzo wysoka\",\n",
    "    \"Modele mają gigantyczne problemy z detekcją, gdy jest ciemno\",\n",
    "    \"Tak samo jak wyżej. 'Large' widzi tylko kilka kart, przez krótki okres czasu\",\n",
    "    \"Przy odpowiednim świetle, predykcje wykonywane na bałym tle są dokładne\",\n",
    "    \"Wysoka precyzja i szybkość detekcji dla obu modeli\",\n",
    "    \"Ponownie dobrze to wygląda. 'Large' przejawia jednak po raz kolejny lepszą dokładność\",\n",
    "    \"Tak samo jak wyżej\",\n",
    "]\n",
    "\n",
    "\n",
    "show_videos_html(labels, numbers_in_line, files, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy zaobserwować, że większy model jest znacznie bardziej stabilny i lepiej radzi sobie z ilością kart na ekranie.\n",
    "\n",
    "Dlatego wybieramy właśnie model \"Photos Large\" na ten, który użyjemy w aplikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalny model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skoro wybraliśmy najlepszy model to przenieśmy go do nowego folderu:\n",
    "\n",
    "```python\n",
    "if os.path.exists('../models/final_model'):\n",
    "    shutil.rmtree('../models/final_model')\n",
    "\n",
    "shutil.copytree('../models/photos_large/train_2', '../models/final_model')\n",
    "```\n",
    "\n",
    "<br></br>\n",
    "Oraz go zainicjujmy:\n",
    "\n",
    "```python\n",
    "final_model = YOLO('../models/final_model/weights/best.pt')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konwersja na inne formaty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając finalny model, musimy przekonwertować go na odpowiedni format, czyli w naszym przypadku **TFLite** - format umożliwiający nam funkcjonalność na urządzeniach mobilnych.\n",
    "\n",
    "Przy wykonywaniu tej konwersji otrzymamy jeszcze model w formacie ONNX.\n",
    "\n",
    "Nie wykonamy jednak jednej konwersji, tylko kilka, w zależności od tego jaką rozdzielczość obrazu akceptuje model:\n",
    "\n",
    "- 320x320\n",
    "    ```python\n",
    "    final_model.export(format='tflite', imgsz=320, half=True)\n",
    "    ```\n",
    "\n",
    "- 640x640\n",
    "    ```python\n",
    "    final_model.export(format='tflite', imgsz=640, half=True)\n",
    "    ```\n",
    "\n",
    "- 1024x1024\n",
    "    ```python\n",
    "    final_model.export(format='tflite', imgsz=1024, half=True)\n",
    "    ```\n",
    "\n",
    "- 1600x1600\n",
    "    ```python\n",
    "    final_model.export(format='tflite', imgsz=1600, half=True)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametr `imgsz` oznacza rozdzielczość obrazu wejściowego do modelu, a `half` odpowiada za kwantyzacje na 16-bit.\n",
    "\n",
    "Kwantyzacja konwertuje wagi i aktywacje modelu z wysokiej precyzji (jak 32-bitowe liczby zmiennoprzecinkowe) na niższą precyzję (jak 16-bitowe liczby zmiennoprzecinkowe lub 8-bitowe liczby całkowite). Zmniejszając rozmiar modelu, przyspiesza wnioskowanie. Trening z uwzględnieniem kwantyzacji (QAT) to metoda, w której model jest trenowany z uwzględnieniem kwantyzacji, zachowując dokładność lepiej niż kwantyzacja po treningu. Obsługując kwantyzację podczas fazy uczenia, model uczy się dostosowywać do niższej precyzji, utrzymując wydajność przy jednoczesnym zmniejszeniu wymagań obliczeniowych.\n",
    "\n",
    "Nawet z parametrem `half=True` otrzymujemy dwa modele:\n",
    "- jeden z precyzją 32-bitową (*best_float32.tflite*)\n",
    "- drugi z precyzją 16-bitową (*best_float16.tflite*)\n",
    "\n",
    "Docelowo interesuje nas *best_float16.tflite*, ponieważ waży dwa razy mniej (ok. 85 MB) od *best_float32.tflite* (ok. 170 MB). Jest to dość istotna rzecz w kontekście aplikacji mobilnej.\n",
    "\n",
    "Przenieśmy nasze modele do folderu *data/export/models/`rozdzielczość`*, np. data/export/models/320x320"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predykcje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy jak wcześniejsze predykcje wyglądają na przekonwertowanych modelach. Znowu skorzystamy z funkcji *predict_test_data* oraz *convert_avi_mp4*:\n",
    "\n",
    "```python\n",
    "final_model = YOLO('../models/final_model/weights/best.pt')\n",
    "test_data = '../data/final_test_data'\n",
    "project = 'final_predict'\n",
    "name = 'original'\n",
    "predict_test_data(final_model, test_data, project, name, img_size=1024)\n",
    "\n",
    "\n",
    "final_model_tflite = YOLO('../data/export/models/320x320/best_float16.tflite')\n",
    "project = 'final_predict/tflite'\n",
    "name = '320x320'\n",
    "predict_test_data(final_model_tflite, test_data, project, name, img_size=320)\n",
    "\n",
    "\n",
    "final_model_tflite = YOLO('../data/export/models/640x640/best_float16.tflite')\n",
    "project = 'final_predict/tflite'\n",
    "name = '640x640'\n",
    "predict_test_data(final_model_tflite, test_data, project, name, img_size=640)\n",
    "\n",
    "\n",
    "final_model_tflite = YOLO('../data/export/models/1024x1024/best_float16.tflite')\n",
    "project = 'final_predict/tflite'\n",
    "name = '1024x1024'\n",
    "predict_test_data(final_model_tflite, test_data, project, name, img_size=1024)\n",
    "\n",
    "\n",
    "final_model_tflite = YOLO('../data/export/models/1600x1600/best_float16.tflite')\n",
    "project = 'final_predict/tflite'\n",
    "name = '1600x1600'\n",
    "predict_test_data(final_model_tflite, test_data, project, name, img_size=1600)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folders = [\n",
    "    'final_predict/original',\n",
    "    'final_predict/tflite/320x320',\n",
    "    'final_predict/tflite/640x640',\n",
    "    'final_predict/tflite/1024x1024',\n",
    "    'final_predict/tflite/1600x1600',\n",
    "]\n",
    "convert_avi_mp4(folders)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zdjęcia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 100%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">#1</p>\n",
    "    <p style=\"text-align: center;\">Photos Large</p>\n",
    "    <img src=\"notebooks/final_predict/original/img_1.jpg\" alt=\"Image 1\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">320x320</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/320x320/img_1.jpg\" alt=\"Image 1\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">640x640</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/640x640/img_1.jpg\" alt=\"Image 1\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1024x1024</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1024x1024/img_1.jpg\" alt=\"Image 1\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1600x1600</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1600x1600/img_1.jpg\" alt=\"Image 1\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;&emsp;&emsp;\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 100%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">#2</p>\n",
    "    <p style=\"text-align: center;\">Photos Large</p>\n",
    "    <img src=\"notebooks/final_predict/original/img_2.jpg\" alt=\"Image 2\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">320x320</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/320x320/img_2.jpg\" alt=\"Image 2\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">640x640</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/640x640/img_2.jpg\" alt=\"Image 2\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1024x1024</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1024x1024/img_2.jpg\" alt=\"Image 2\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1600x1600</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1600x1600/img_2.jpg\" alt=\"Image 2\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;&emsp;\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 100%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">#3</p>\n",
    "    <p style=\"text-align: center;\">Photos Large</p>\n",
    "    <img src=\"notebooks/final_predict/original/img_3.jpg\" alt=\"Image 3\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">320x320</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/320x320/img_3.jpg\" alt=\"Image 3\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">640x640</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/640x640/img_3.jpg\" alt=\"Image 3\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1024x1024</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1024x1024/img_3.jpg\" alt=\"Image 3\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1600x1600</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1600x1600/img_3.jpg\" alt=\"Image 3\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;&emsp;\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 100%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">#4</p>\n",
    "    <p style=\"text-align: center;\">Photos Large</p>\n",
    "    <img src=\"notebooks/final_predict/original/img_4.jpg\" alt=\"Image 4\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">320x320</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/320x320/img_4.jpg\" alt=\"Image 4\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">640x640</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/640x640/img_4.jpg\" alt=\"Image 4\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1024x1024</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1024x1024/img_4.jpg\" alt=\"Image 4\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1600x1600</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1600x1600/img_4.jpg\" alt=\"Image 4\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;&emsp;\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 100%;\">\n",
    "    <p style=\"font-weight: bold; text-align: center;\">#5</p>\n",
    "    <p style=\"text-align: center;\">Photos Large</p>\n",
    "    <img src=\"notebooks/final_predict/original/img_5.jpg\" alt=\"Image 5\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">320x320</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/320x320/img_5.jpg\" alt=\"Image 5\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">640x640</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/640x640/img_5.jpg\" alt=\"Image 5\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;\n",
    "<div style=\"display: flex;\">\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1024x1024</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1024x1024/img_5.jpg\" alt=\"Image 5\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "  <div style=\"text-align: center; width: 50%;\">\n",
    "    <p style=\"text-align: center;\">1600x1600</p>\n",
    "    <img src=\"notebooks/final_predict/tflite/1600x1600/img_5.jpg\" alt=\"Image 5\" style=\"height: 400px;\">\n",
    "  </div>\n",
    "</div>\n",
    "&emsp;&emsp;\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że model **320x320** niezbyt dobrze sobie radzi, ale poza nim wyniki są mocno satysfakcjonujące."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filmy wideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "def show_videos_html_5(numbers_in_line, files, description):\n",
    "  lines = len(files)//(sum(numbers_in_line))\n",
    "  first = True\n",
    "\n",
    "  html_str = \"\"\"\"\"\"\n",
    "\n",
    "  for k in range(lines):\n",
    "\n",
    "    for num_in_line in numbers_in_line:\n",
    "      if num_in_line == 1:\n",
    "        height = '400px'\n",
    "        width = '100%'\n",
    "        labels = ['Photos Large']\n",
    "        filepaths = files[k*sum(numbers_in_line) : k*sum(numbers_in_line)+1]\n",
    "\n",
    "      elif num_in_line == 2:\n",
    "        height = '400px'\n",
    "        width = '50%'\n",
    "        if first:\n",
    "          first = False\n",
    "          labels = ['320x320', '640x640']\n",
    "          filepaths = files[k*sum(numbers_in_line)+1 : k*sum(numbers_in_line)+3]\n",
    "        else:\n",
    "          first = True\n",
    "          labels = ['1024x1024', '1600x1600']\n",
    "          filepaths = files[k*sum(numbers_in_line)+3 : k*sum(numbers_in_line)+5]\n",
    "\n",
    "      \n",
    "      html_str += \"\"\"<div style=\"display: flex; font-size: 17px\">\"\"\"\n",
    "      for i, file in enumerate(filepaths):\n",
    "          mp4 = open(file,'rb').read()\n",
    "          data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "          html_str += \"\"\"<div style=\"text-align: center; width: %s;\">\"\"\" % str(width)\n",
    "          if num_in_line == 1:\n",
    "            html_str += \"\"\"<p style=\"font-weight: bold; text-align: center;\">#%s</p>\"\"\" % str(k+1)\n",
    "          html_str += \"\"\"<p style=\"text-align: center;\">%s</p>\"\"\" % labels[i]\n",
    "          html_str += \"\"\"\n",
    "          <video controls style=\"height: %s;\" src=\"%s\"></video>\n",
    "          \"\"\" % (str(height), data_url)\n",
    "          html_str += \"\"\"</div>\"\"\"\n",
    "          \n",
    "      html_str += \"\"\"</div>\"\"\"\n",
    "      html_str += \"\"\"<p style=\"font-size: 17px\">%s</p>\"\"\" % description[k]\n",
    "      html_str += \"\"\"&emsp;\"\"\"\n",
    "\n",
    "    html_str += \"\"\"&emsp;&emsp;\"\"\"\n",
    "\n",
    "  return HTML(html_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "numbers_in_line = (1, 2, 2)\n",
    "\n",
    "files = [\n",
    "    \"notebooks/final_predict/original/vid_3.mp4\",\n",
    "    \"notebooks/final_predict/tflite/320x320/vid_3.mp4\", \"notebooks/final_predict/tflite/640x640/vid_3.mp4\",\n",
    "    \"notebooks/final_predict/tflite/1024x1024/vid_3.mp4\", \"notebooks/final_predict/tflite/1600x1600/vid_3.mp4\",\n",
    "\n",
    "    # \"notebooks/final_predict/original/vid_10.mp4\",\n",
    "    # \"notebooks/final_predict/tflite/320x320/vid_10.mp4\", \"notebooks/final_predict/tflite/640x640/vid_10.mp4\",\n",
    "    # \"notebooks/final_predict/tflite/1024x1024/vid_10.mp4\", \"notebooks/final_predict/tflite/1600x1600/vid_10.mp4\",\n",
    "\n",
    "    \"notebooks/final_predict/original/vid_14.mp4\",\n",
    "    \"notebooks/final_predict/tflite/320x320/vid_14.mp4\", \"notebooks/final_predict/tflite/640x640/vid_14.mp4\",\n",
    "    \"notebooks/final_predict/tflite/1024x1024/vid_14.mp4\", \"notebooks/final_predict/tflite/1600x1600/vid_14.mp4\",\n",
    "]\n",
    "\n",
    "description = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "\n",
    "show_videos_html_5(numbers_in_line, files, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widzimy różnice w wyeksportowanych modelach względem orginału są minimalne, ba czasami są one lepsze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architektura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobaczmy na koniec jak wygląda architektura naszego finalnego modelu:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "  <img src='notebooks/architecture/final_model_pt.png' width = '90%' alt=\"Architecture graph\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym notatniku przeszliśmy przez najważniejsze kroki podczas tworzenia modelu rozpoznawania obiektów:\n",
    "1. Utworzenie modelu\n",
    "\n",
    "2. Uczenie na naszym zbiorze\n",
    "\n",
    "3. Walidacja przy użyciu zbioru walidacyjnego oraz testowego\n",
    "\n",
    "4. Predykcja na realnych zdjęciach i filmach wideo\n",
    "\n",
    "5. Wybór najlepszego modelu spośród kilku\n",
    "\n",
    "6. Eksport modelu do różnych formatów\n",
    "\n",
    "7. Test wyeksportowanych modeli\n",
    "\n",
    "8. Zbadanie architektury modelu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Zakończyliśmy pracę w tym notatniku. Teraz pozostało już tylko utworzyć aplikację mobilną."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert\n",
    "# jupyter nbconvert --to html_embed --Exporter.preprocessors=[\\\"data.preprocessor.hide_code_preprocessor.HideCodePreprocessor\\\"] yolov8_models.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
